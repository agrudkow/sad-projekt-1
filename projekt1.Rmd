---
title: "Projekt 1"
author: "Banzekulivakha Zhan, Grudkowski Artur"
date: "18 04 2021"
output: html_document
---

## Struktura projektu
Struktura katalogów w projekcie przedstawia się nastepująco:

* data - katalog przechowyjący dane
* info - katalog przechowujący pliki z treścią zadania
* renv - katalog odpowiedzialny ze wirtulane środowisko języka R
* projekt1.Rmd - główny plik (notatnik) zawierajcy kod źródłowy
* projekt1.html - plik wynikowy projekt1.Rmd 
* inne pliki konfiguracjne

## Wyamgane biblioteki i konfiguracja pakeitu *knit*
> Poniżej linkwoane bibliteki są wymagane do poprawnej "kompilacji" notatnika, stąd też wymagana jest ich wcześniejsza instalacja.

```{r setup, warning=FALSE, message=FALSE}
library(tidyverse)

knitr::opts_chunk$set(echo = TRUE)
```

## Zadanie 1

### Przygotowanie danych
Sekcja ta definiuje funkcje odpowiedzilne za odpowiednie wczytanie danych i ich przygotowanie. 

* load_data - wczytuje dane z pliku do tabeli (data frame-u)
* prepare data:
  + zmienia nazwy kolumn - ma to na celu poprawić czytelność kodu
  + wydziela przedział czasu, którego dotyczny zadanie (druga połowa 2019) 

```{r}
load_data <- function(path) {
  return(read.table(path, sep = ",", header = TRUE))
}

prepare_data <- function(df) {
  # Change col names
  col_names <- c( "TICKER", "DATE", "OPEN", "HIGH", "LOW", "CLOSE", "VOL")
  colnames(df) <- col_names

  # Filter data to suit assagnment details (transactions in second half of 2019)
  df <- df[df$DATE >= 20190701 & df$DATE <= 20191231,]
  
  return(df[order(df$DATE),])
}
```

### Dla zadanych spółek notowanych na WGPW, na podstawie ich notowan z drugiej połowy roku 2019 (z pliku zad1mst.zip):

### (a) wyznacz procentowe zmiany cen zamkniecia tych spółek

Zmiany procentowe są interpretowane jako zmiany zamknięcia między dwoma następującymi notowaniami w dwóch kolejnych dniach.
Wynik dla notowania *i-1* jest liczny ze wzoru: $diff = (n(i - 1) - n(i)/n(i - 1)$, gdzie *n* oznacza zbiór notowań.

* calc_close_diff - zwraca zmian procentowe cen zamknięcia wyliczone na podstawie schematu przedstawionego powyżej

```{r}
calc_close_diff <- function(dataset) {
  # Init empty vector
  close_diff <- c()

  for(i in 2:nrow(dataset)) {
    first_day_val <- dataset$CLOSE[i - 1]
    second_day_val <- dataset$CLOSE[i]

    diff <- (first_day_val - second_day_val) / first_day_val
    # Convert to percentage
    diff <- diff * 100
    close_diff <- c(close_diff, diff)
  }

  return(close_diff)
}
```

#### Zmiany procentowe dla dla spółki PLAY
```{r}
# PLAY
close_diff_play <- load_data("data/p1mst/PLAY.mst") %>% prepare_data %>% calc_close_diff

print(close_diff_play)
```

#### Zmiany procentowe dla dla spółki LPP
```{r}
# LPP
close_diff_lpp <- load_data("data/p1mst/LPP.mst") %>% prepare_data %>% calc_close_diff

print(close_diff_lpp)
```

### (b) zilustruj rozkłady ww. zmian (histogramy + wykresy pudełkowe)

* draw_plots - generuje histogram i wykres pudełkowy dla zadanych danych zmian cen zamkniecia spółek

```{r draw_plots}
draw_plots <- function(data) {
  hist(data, main="Histogram rozkładu procentowej zmiany cen zamknięcia", xlab="Zmiana procentowa", ylab="Liczność", col="darkmagenta", breaks=50)
  boxplot(data, main="Wykres pudełkowy rozkładu procentowej zmiany cen zamknięcia", xlab = "Zmiana procentowa", horizontal = TRUE)
  axis(side=1, at=seq(as.integer(min(data))-1,as.integer(max(data))+1, 1))
}

```

#### Wykres dla spółki PLAY
```{r}
draw_plots(close_diff_play)
```

#### Wykres dla spółi LPP
```{r}
draw_plots(close_diff_lpp)
```

### (c) wyestymuj parametry rozkładów normalnych mogacych modelowac ww. rozkłady

 * estimate_dnorm_params - zwraca listę zawierającą średnia i odchylenie standardowe wyliczone na podstawie danych wejściowych

```{r}
estimate_dnorm_params <- function(data) {
  return(list(mean=mean(data), sd=sd(data)))
}
```

#### Estymacja dla spółki PLAY
```{r}
dnorm_params_play <- estimate_dnorm_params(close_diff_play)

cat("Średnia: ", dnorm_params_play$mean)
cat("Odchylenie standardowe: ", dnorm_params_play$sd)
```

#### Estymacja dla spółki LPP
```{r}
dnorm_params_lpp <- estimate_dnorm_params(close_diff_lpp)

cat("Średnia: ", dnorm_params_lpp$mean)
cat("Odchylenie standardowe: ", dnorm_params_lpp$sd)
```

### (d) porównaj graficznie rozkłady modelowe z danymi
Porównanie zostało zrealizowanie poprzez połącznie wykresu uzykanego (wyestymowanego) rozkładu normalnego wraz z histogramem (sekcja 1 b).

* plot_comparison - generuje, wspomnien wyżej, połącznie dwóch wykresów

```{r}
plot_comparison <- function(data, dnorm_params, col="darkmagenta") {
  hist <- hist(data, main="Porównanie rozkładu modelowego (normalnego) z danymi", xlab="Zmiana procentowa", ylab="Liczność (wartość funkcji gęstości)", col=col, breaks=50) 
  axis(side=1, at=seq(as.integer(min(data)) - 1, as.integer(max(data)) + 1, 1))
  xfit <- seq(min(data), max(data), length = 40) 
  yfit <- dnorm(xfit, mean = dnorm_params$mean, sd = dnorm_params$sd) 
  yfit <- yfit * diff(hist$mids[1:2]) * length(data) 
  lines(xfit, yfit, col = "black", lwd = 2)
}
```

#### Porównanie dla spółki PLAY
```{r}
plot_comparison(close_diff_play, dnorm_params_play)
```

#### Porównanie dla spółki LPP
```{r}
plot_comparison(close_diff_lpp, dnorm_params_lpp)
```

#### Komentarz
Na podstawie wyżej wygenerowanych porównań jesteśy w stanie stwierdzić, iż estymacja przebiegła pomyślnie, ponieważ charakterystyka estymowanych rozkładów normalnych odpowada charakterystyce histogramów.

Dodatowo można zauważyć, iż spółka LPP cechowała się większą stabilnością na giełdzie. Świadczy o tym mniejsze odchylenie standardowe zmian procentowych niż jest to obecne w danych dla spółki PLAY.

## Zadanie 2

Sekcja ta definiuje funkcje odpowiedzilne za odpowiednie wczytanie danych i ich przygotowanie. 

* load_data - wczytuje dane z pliku do tabeli (data frame-u)
* prepare data:
  + wydzielenie przedział czasu, którego dotyczny zadanie (październik 2019) 
  + dodanie kolumny *id* - ma to na celu umożliwienie odtworzenia pierwotnej kolejności danych (transakcji) oraz unikane zidentyfikowanie transakcji

### Przygotowanie danych

```{r}
load_data <- function(path) {
  return(read.table(path, sep = ",", header = TRUE))
}

prepare_data <- function(df) {
  # Filter data to suit assagnment details (transactions in October 2019)
  df <- df[df$date >= 191001 & df$date <= 191031,]
  
  # Add id column
  data <- tibble::rowid_to_column(df, "id")

  return (data)
}
```


### Na podstawie danych z pazdziernika 2019 (z pliku zad2csv new.zip) dotyczacych zadanej spółki:

### (a) Zilustruj jak wolumen transakcji rozkłada sie pomiedzy 3 fazy notowan: otwarcie, notowania ciagłe, zamkniecie z dogrywka.

Badanie rozkłądu wolumenów transakcji jest interpretowane jak wyliczanie warotści średniej wolumenów transkacji dla każdego dnia w danej grupie (fazie) notowań.

* extract_opening_transactions - wydziela transakcje należące do grupy otwarcia
  + wyznacznie pierwyszych transkcji o godzinie 9 dla każdego dnia
  + wyszukanie transakcji, które mają tę samą cenę co pierwsza transakcja z godziny 9
* extract_continous_transactions - wydziela transakcje należące do grupy notowań ciągłych
  + wyznacznie tych transakcji, które nie należą do fazy otwarcia i wystąpiły do 16:51
* extract_closing_transactions - wydziela transakcje należące do grupy notowań zamknięcia z dogrywką
  + wyznacznie tych transakcji, które wystąpiły po 16:59
* extract_transaction_groups - wywołuje poprzednie funkcje i agreguje wyniki w listę

```{r}
extract_opening_transactions <- function(df) {
  # Select first opening transaction for each day
  first_opening_transactions <- df[df$time == 90000,] %>% group_by(date) %>% slice(1)

  # Init opening transactions
  opening_transactions <- data.frame(first_opening_transactions)

  # Select related transactions for each of first opening transactions
  for (i in 1:nrow(first_opening_transactions)) {
    result <- data.frame(df[df$date == first_opening_transactions$date[i] 
              & df$time == first_opening_transactions$time[i] 
              & df$price == first_opening_transactions$price[i] 
              & df$id != first_opening_transactions$id[i],]) # Skip transactions from first_opening_transactions

    opening_transactions <- rbind(opening_transactions, result)
  }

  return(opening_transactions[order(opening_transactions$id),])
}

extract_continous_transactions <- function(df, opening_transactions) {
  return(df[df$time <= 165100 & !(df$id %in% opening_transactions$id),])
}

extract_closing_transactions <- function(df, opening_transactions) {
  return(df[df$time >= 165900,])
}

extract_transaction_groups <- function(df) {
  opening_transactions <- extract_opening_transactions(df)
  continous_transactions <- extract_continous_transactions(df, opening_transactions)
  closing_transactions <- extract_closing_transactions(df)

  return(list(opening=opening_transactions, continous=continous_transactions, closing=closing_transactions))
}
```

* avg_volumes_by_day - wylicza wartość wolumen dla każdego dnia

```{r}
avg_volumes_by_day <- function(dataset) {
  return(dataset %>% group_by(date) %>% summarise(avg_volume = mean(volume)))
}

```

#### Wydzielenie grup

```{r}
transactions <- load_data("data/p1csv_new/PGE.csv") %>% prepare_data %>% extract_transaction_groups

opening <- data.frame(group="opening", avg_volumes_by_day(transactions$opening) %>% select(avg_volume, date))
continous <- data.frame(group="continous", avg_volumes_by_day(transactions$continous) %>% select(avg_volume, date))
closing <- data.frame(group="closing", avg_volumes_by_day(transactions$closing) %>% select(avg_volume, date))
```

##### Cztery dni o największej wartości średniej wolumentów dla każdej z grup
```{r}
print(opening %>% arrange(-avg_volume) %>% slice(1:4))
print(continous %>% arrange(-avg_volume) %>% slice(1:4))
print(closing %>% arrange(-avg_volume) %>% slice(1:4))
```

#### Rozkład wolumenu transakcji fazy otwarcia (wykres pudełkowy)

```{r}
generate_volume_dist_plot <- function(df) {
  ggplot(df, aes(x=avg_volume)) + 
    geom_boxplot() + 
geom_boxplot() + 
    geom_boxplot() + 
    theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) +
    labs(title = "Rozkład średniej wartości wolumenu transakcji", y="", x="Śerednia wartość wolumentów (w ciągu dnia)")
}
```
```{r}
generate_volume_dist_plot(opening)
```

#### Rozkład wolumenu transakcji fazy notowań ciagłego (wykres pudełkowy)
```{r}
generate_volume_dist_plot(continous)
```

#### Rozkład wolumenu transakcji fazy zamknięcia (wykres pudełkowy)
```{r}
generate_volume_dist_plot(closing)
```

#### Porównanie wyżej wymienionych rozkładów (wykresy pudełkowe)
```{r}
ggplot(rbind(opening, continous, closing), aes(x=group, y=avg_volume, fill=group)) + 
geom_boxplot() + 
labs(title = "Porównanie wyżej wymienionych  rozkładów", x="Grupy (fazy)", y="Średnia wartość wolumentów", color="Grupy (fazy)")
```

#### Komentarz
Jak widać na przedstawionym porównaniu, grupa notowań zamknięcia z dogrywką ma największą średnią wartość wolumentów. Interpretacja tego może być następująca: inwestorzy najchętniej (w największych ilościach) dokonują obrotów papaierami wartościowymi pod koniec dnia.
Dodatkowo można zauważyć ciekawą sytacje w przypadku największej wartości średniej dla fazy otwarcia i zamknięcia. Mianowicie największa warość dla fazy zamknięcia poprzedza największą wartość dla fazy otwarcia. W naszym przypadku mowa o zmknięciu w dniu 10.10.2019 i 
otwarciu w dniu 11.10.2019. Można się zatem pokusić o stwierdzenie, że pod koniec dnia 10.10.2019 coś niestandardowego się zadziało w kontekście gospodarki dla grupy PGE i miało to wpływ na duży wzrost dynamiki tranaskcji. Co się zadziało? Źródła podają, że PGE ukończło 
jakąś dużą inwestycję.

Oczywistym jest, że gdyby zliczana była suma, a nie wartość średnia, wolumentów transakcji to faza notowań ciągły wyróżniałaby się ponad inne fazy ze względu na znaczną różńice w ilości transakcji tj. faza ta ma znacznie więcej transakcji niż pozostałe fazy.

### (b) zilustruj jak wolumen transakcji rozkłada sie w czasie notowan ciagłych
W celu ilustarcji rozkładu wolumentu transakcji w czasie, grupa notowań ciągłych została podzielnoa na godziny a następnie zliczona została suma wolumentów w każdej godzinie.

* extract_hours - dodaje kolumne *hour* z pełną godziną na podstawie kolumny *time*

```{r}
extract_hours <- function(df) {
  for (i in 1:nrow(df)) {
     df$hour[i] = df$time[i] %/% 10000
  }
  return(df)
}
```

* avg_volumes_by_hour - wylicza wartość średnią wolumentów transakcji dla każdej godziny

```{r}
avg_volumes_by_hour <- function(df) {
  return(df %>% group_by(hour) %>% summarise(avg_volume = mean(volume)))
}
```

#### Rozkład wolumanu transakcji w czasie
Rozkład wolumenu transkacji w czasie został zilustrowany przy pomocy histogramu

```{r}
volumes_sum_by_hour <- transactions$continous %>% extract_hours %>% avg_volumes_by_hour
g <- ggplot(volumes_sum_by_hour, aes(x=hour, y=avg_volume))
g + geom_col() + labs(title = "Histogram rozkładu sredniej wartości wolumenów dla każdej godziny", x="Godzina", y="Średnia wartość wolumentów")
```

#### Komentarz
Na podstawie wygenerowanego rozkładu sredniej wartości wolumenu transakcji w czasie notowań ciągłych można stwierdzić, że po lekkim zwięszeniu średniej warości wolumentów w godzinnach porannych (9-10), 
następuje lekki spadek i stabilizacja do ok. godz. 14. W okolicach godz. 14 notowany jest wzrost sumy wolumenów po czym następuję lekki spadek do godz. 16. Jednakże, w ogólności można przyjąć, że rozkład ten jest stały.

### (c) Wyznacz dzień i jego 2-godzinny przedział czasu rozpoczynający się o pełnej godzinie między 10:00 a 14:00 włącznie, w którym jest najmniej sekund, w których przeprowadzane były transakcje na akcjach danej spółki.


### Przygotowanie danych
* count_used_secs_by_hour - na podstawie wczytywanych danych wylicza ilość użytych sekund na tranzakcję w ciągu jednej godziny
* extract_2h_time_frame_min_secs - wyszukuję przedział czasu rozpoczynający się o pełnej godzinie mędzy 10 a 14, w którym jest najmniej sekund

```{r}
count_used_secs_by_hour <- function(df) {
  return(df %>% group_by(date, hour) %>% summarise(n_distinct(time)))
}

extract_2h_time_frame_min_secs <- function(df) {
  min_secs = 99999999
  start_hour = 0
  date = 0
  for (i in 1:(nrow(df)-1)) {
    count_for_2h <- df$count[[i]] + df$count[[i+1]]
    if(count_for_2h < min_secs){
      date <- df$date[i]
      start_hour <- df$hour[i]
      min_secs <- count_for_2h
    }
  }
  
  return(list(date=date, start_hour=start_hour, min_secs=min_secs))
}

transactions_by_hour <- load_data("data/p1csv_new/PGE.csv") %>% prepare_data %>% filter(time >= 100000, time < 160000) %>% extract_hours %>% count_used_secs_by_hour
names(transactions_by_hour)[3] <- "count"

min_secs_time_frame <- extract_2h_time_frame_min_secs(transactions_by_hour)
min_secs_date <- min_secs_time_frame$date
min_secs_start_time <- min_secs_time_frame$start_hour * 10000 # z
min_secs_finish_time <- min_secs_start_time + 20000 # plus 2 hours
min_secs_data_df <- load_data("data/p1csv_new/PGE.csv") %>% 
                    prepare_data %>% 
                    filter(time >= min_secs_start_time, time <= min_secs_finish_time, date==min_secs_date) %>% 
                    distinct(time, .keep_all = TRUE)
```


#### (i). W każdej kolejnej minucie rozważanego przedziału wyznacz liczbę sekund, w których przeprowadzane były transakcje (otrzyma się w ten sposób 120 liczb z przedziału [0, 60]).

* extract_minutes_start_from - zwraca wektor wszystkich minut z minimalnego przedziału
* count_used_secs_per_min - wylicza ilość użytych sekund na tranzakcję w każdej minucie

```{r}
extract_minutes_start_from <- function(start_time) {
  times <- c(as.integer(start_time / 100)) # 2 - start time
  actual_minute <- times[1]
  for(i in 1:120) {
    actual_minute <- actual_minute + 1 # plus 1 min
    if(str_detect(actual_minute, "60$")) { # next hour
      actual_minute <- actual_minute + 40
    }
    times <- c(times, actual_minute)
  }

  return(times)
}

count_used_secs_per_min <- function(start_time, df) {
  times <- extract_minutes_start_from(start_time)
  counts <- c() 
  for(i in 1:length(times)) {
    time <- times[i]
    pattern <- paste("^", time, sep = "")
    num_used_secs <- df %>% filter(str_detect(time, pattern)) %>% nrow
    counts <- c(counts, num_used_secs)
  }
  
  return(data.frame(times, counts))
}

df_count_per_minute <- count_used_secs_per_min(min_secs_start_time, min_secs_data_df)
df_count_per_minute$minute <- seq.int(0:120)
```

* draw_plot - rysuje wykres ilustrujący zależność użytych secund na transakcję od minuty.

```{r}
draw_plot <- function(df) {
  ggplot(data = df) +
    geom_bar(aes(x = minute,
                 y = counts),
             stat="identity") +
    scale_x_continuous(breaks=seq(0,120,5)) +
    scale_y_continuous(breaks=seq(min(df$counts),
                                  max(df$counts),
                                   1)) +
    labs(x = 'Minuta',
         y = 'Ilosc sekund',
         title = 'Liczba uzytych sekund na transakcje w ciągu minuty') +
    theme_bw()
}

draw_plot(df_count_per_minute)
```


#### (ii). Zamodeluj ww. liczby za pomocą rozkładu Poissona.

* draw_poisson_plot - rysuje wykres ilustrujący funkcję rozkładu prawdopodobieństwa *(P(x = k))* oraz dystrubuantę *(P(x <= k))*

```{r}
draw_poisson_plot <- function(df, x_data) {
  lambda = mean(df$counts)
  density <- dpois(x = x_data, lambda)
  prob <- ppois(q = x_data, lambda, lower.tail = TRUE)
  df <- data.frame(x_data, density, prob)
  ggplot(df, aes(x = x_data, y = density)) +
    scale_x_continuous(breaks=seq(0,10,1)) +
    geom_text(aes(label = round(density,2)), size=3, vjust=-1) +
    labs(title = " Rozkład prawdopodobieństwa oraz dystrubuanta",
         x = "Liczba transakcji na minute",
         y = "") +
    geom_line(data = df, aes(x = x_data, y = prob, color="Dystrubuanta")) +
    geom_line(data = df, aes(x = x_data, y = density, color="Rozkład Poissona")) +
    geom_point(aes(x = x_data, y = density, ), size=2 )
}

num_transactions <- 0:10
draw_poisson_plot(df_count_per_minute, num_transactions)
```

#### (iii). porównaj rozkład modelowy z danymi.

```{r}
plot_comparison_hist_and_poisson_mass_function <- function(data, x_data, y_data) {
  hist <- hist(data, main="Histogram rozkłądu ilości tranzakcji w minutę", xlab="Liczba uzytych sekund w minute", ylab="Liczność", col="darkmagenta", ylim=c(0,30), breaks=50) 
  xfit <- x_data
  yfit <- y_data
  yfit <- yfit * diff(hist$mids[1:2]) * length(data) 
  lines(xfit, yfit, col = "red", lwd = 2)
}

density <- dpois(num_transactions, mean(df_count_per_minute$counts))
plot_comparison_hist_and_poisson_mass_function(df_count_per_minute$counts, num_transactions, density)
```

#### Komentarz
Na podstawie wyżej wygenerowanym porównaniu jesteśmy w stanie stwierdzić, iż estymacja przebiegła pomyślnie, jednak liczba danych (liczba użytych sekund) jest zbyt mała, żeby coś więcej wskazać.

    